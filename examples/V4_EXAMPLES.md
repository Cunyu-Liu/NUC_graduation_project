# v4.0é™¢å£«çº§ç§‘ç ”æ™ºèƒ½åŠ©æ‰‹ - å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### å®‰è£…ä¾èµ–
```bash
# å®‰è£…Pythonä¾èµ–
pip install sqlalchemy psycopg2-binary asyncio aiofiles

# å®‰è£…LLMä¾èµ–
pip install langchain langchain-openai

# å®‰è£…PDFå¤„ç†
pip install PyMuPDF pdfplumber

# å®‰è£…å¼‚æ­¥æ”¯æŒ
pip install asyncio
```

#### æ•°æ®åº“é…ç½®
```bash
# å®‰è£…PostgreSQL
# macOS
brew install postgresql
brew services start postgresql

# åˆ›å»ºæ•°æ®åº“
createdb literature_analysis

# é…ç½®ç¯å¢ƒå˜é‡
export DATABASE_URL="postgresql://user:password@localhost:5432/literature_analysis"
export GLM_API_KEY="your-api-key"
```

---

## ğŸ“– æ ¸å¿ƒåŠŸèƒ½ç¤ºä¾‹

### ç¤ºä¾‹1: å•ç¯‡è®ºæ–‡å®Œæ•´å·¥ä½œæµ

```python
import asyncio
from src.db_manager import DatabaseManager
from src.async_workflow import AsyncWorkflowEngine

async def analyze_single_paper():
    """åˆ†æå•ç¯‡è®ºæ–‡çš„å®Œæ•´å·¥ä½œæµ"""

    # åˆå§‹åŒ–
    db = DatabaseManager()
    db.create_tables()  # é¦–æ¬¡è¿è¡Œåˆ›å»ºè¡¨

    workflow = AsyncWorkflowEngine(
        db_manager=db,
        llm_config={
            'model': 'glm-4-plus',
            'api_key': 'your-api-key',
            'max_concurrent': 5
        }
    )

    # æ‰§è¡Œå·¥ä½œæµ
    result = await workflow.execute_paper_workflow(
        pdf_path='papers/deep_learning_paper.pdf',
        tasks=['summary', 'keypoints', 'topic', 'gaps', 'graph', 'code'],
        auto_generate_code=True
    )

    # æŸ¥çœ‹ç»“æœ
    print(f"\nå·¥ä½œæµç»“æœ:")
    print(f"  çŠ¶æ€: {result['status']}")
    print(f"  è®ºæ–‡ID: {result['paper_id']}")
    print(f"  åˆ†æID: {result['analysis_id']}")
    print(f"  å·²å®Œæˆä»»åŠ¡: {result['tasks_completed']}")
    print(f"  ç ”ç©¶ç©ºç™½æ•°: {result.get('gaps_count', 0)}")
    print(f"  ç”Ÿæˆä»£ç æ•°: {result.get('code_generated', 0)}")
    print(f"  æ€»è€—æ—¶: {result['duration']:.2f}ç§’")

    return result

# è¿è¡Œ
result = asyncio.run(analyze_single_paper())
```

### ç¤ºä¾‹2: æ‰¹é‡å¤„ç†è®ºæ–‡

```python
async def batch_analyze_papers():
    """æ‰¹é‡åˆ†æå¤šç¯‡è®ºæ–‡"""

    db = DatabaseManager()
    workflow = AsyncWorkflowEngine(db_manager=db)

    # è¦å¤„ç†çš„è®ºæ–‡åˆ—è¡¨
    pdf_files = [
        'papers/paper1.pdf',
        'papers/paper2.pdf',
        'papers/paper3.pdf',
        'papers/paper4.pdf',
        'papers/paper5.pdf',
    ]

    # æ‰¹é‡å¤„ç†ï¼ˆè‡ªåŠ¨å¹¶å‘ï¼‰
    summary = await workflow.batch_process_papers(
        pdf_paths=pdf_files,
        tasks=['summary', 'keypoints', 'gaps', 'code']
    )

    print(f"\næ‰¹é‡å¤„ç†å®Œæˆ:")
    print(f"  æ€»æ•°: {summary['total']}")
    print(f"  æˆåŠŸ: {summary['success']}")
    print(f"  å¤±è´¥: {summary['failed']}")
    print(f"  æ€»è€—æ—¶: {summary['duration']:.2f}ç§’")
    print(f"  å¹³å‡æ—¶é—´: {summary['avg_time']:.2f}ç§’/ç¯‡")

    return summary

# è¿è¡Œ
summary = asyncio.run(batch_analyze_papers())
```

### ç¤ºä¾‹3: æ•°æ®åº“CRUDæ“ä½œ

```python
from src.db_manager import DatabaseManager

def database_crud_example():
    """æ•°æ®åº“CRUDæ“ä½œç¤ºä¾‹"""

    db = DatabaseManager()

    # 1. åˆ›å»ºï¼ˆCreateï¼‰
    paper_data = {
        'title': 'Attention Is All You Need',
        'abstract': 'The dominant sequence transduction models...',
        'year': 2017,
        'venue': 'NeurIPS',
        'pdf_path': '/path/to/attention.pdf',
        'pdf_hash': 'abc123',
        'authors': [
            {'name': 'Ashish Vaswani'},
            {'name': 'Noam Shazeer'}
        ],
        'keywords': ['attention', 'transformer', 'neural networks']
    }

    paper = db.create_paper(paper_data)
    print(f"âœ“ åˆ›å»ºè®ºæ–‡: ID={paper.id}")

    # 2. è¯»å–ï¼ˆReadï¼‰
    paper = db.get_paper(paper.id)
    print(f"âœ“ æŸ¥è¯¢è®ºæ–‡: {paper.title}")

    # æœç´¢è®ºæ–‡
    papers = db.get_papers(search='transformer', year_from=2017)
    print(f"âœ“ æœç´¢ç»“æœ: {len(papers)} ç¯‡")

    # 3. æ›´æ–°ï¼ˆUpdateï¼‰
    updated_paper = db.update_paper(
        paper.id,
        {'doi': '10.5555/12345'}
    )
    print(f"âœ“ æ›´æ–°è®ºæ–‡: DOI={updated_paper.doi}")

    # 4. åˆ é™¤ï¼ˆDeleteï¼‰
    success = db.delete_paper(paper.id)
    print(f"âœ“ åˆ é™¤è®ºæ–‡: {success}")

    # æ‰¹é‡åˆ é™¤
    success = db.batch_delete_papers([1, 2, 3])
    print(f"âœ“ æ‰¹é‡åˆ é™¤: {success} ç¯‡")

# è¿è¡Œ
database_crud_example()
```

### ç¤ºä¾‹4: ä»£ç ç”Ÿæˆä¸äº¤äº’

```python
import asyncio
from src.code_generator import CodeGenerator
from src.db_manager import DatabaseManager
from src.database import ResearchGap

async def code_generation_example():
    """ä»£ç ç”Ÿæˆç¤ºä¾‹"""

    db = DatabaseManager()
    generator = CodeGenerator(db_manager=db)

    # åˆ›å»ºç ”ç©¶ç©ºç™½
    gap_data = {
        'analysis_id': 1,  # å‡è®¾æœ‰ä¸€ä¸ªåˆ†æID=1
        'gap_type': 'methodological',
        'description': 'ç°æœ‰å›¾ç¥ç»ç½‘ç»œåœ¨å¤§è§„æ¨¡å›¾ä¸Šçš„è®¡ç®—å¤æ‚åº¦æ˜¯O(VÂ²)ï¼Œæ— æ³•å¤„ç†åƒä¸‡çº§èŠ‚ç‚¹',
        'importance': 'high',
        'difficulty': 'medium',
        'potential_approach': 'è®¾è®¡åŸºäºç¨€ç–çŸ©é˜µçš„çº¿æ€§å¤æ‚åº¦å›¾èšåˆç®—æ³•',
        'expected_impact': 'å¯ä»¥å°†å›¾ç¥ç»ç½‘ç»œçš„åº”ç”¨è§„æ¨¡æ‰©å¤§10å€'
    }

    gap = db.create_research_gap(gap_data)

    # ç”Ÿæˆä»£ç 
    print("æ­£åœ¨ç”Ÿæˆä»£ç ...")
    code_data = await generator.generate_code_async(
        research_gap=gap,
        strategy='method_improvement',
        language='python',
        framework='pytorch'
    )

    print(f"\nâœ“ ä»£ç ç”Ÿæˆå®Œæˆ:")
    print(f"  è¯­è¨€: {code_data['language']}")
    print(f"  æ¡†æ¶: {code_data['framework']}")
    print(f"  è´¨é‡è¯„åˆ†: {code_data['quality_score']:.2f}")
    print(f"  ä»£ç é•¿åº¦: {len(code_data['code'])} å­—ç¬¦")

    # ä¿å­˜åˆ°æ•°æ®åº“
    code_data['gap_id'] = gap.id
    code_record = db.create_generated_code(code_data)
    print(f"  ä»£ç ID: {code_record.id}")

    # ç”¨æˆ·ä¿®æ”¹ä»£ç 
    print("\nç”¨æˆ·è¦æ±‚ä¼˜åŒ–ä»£ç ...")
    user_prompt = "è¯·ä¼˜åŒ–ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Š"

    updated_code = await generator.modify_code_async(
        code_id=code_record.id,
        user_prompt=user_prompt,
        db_manager=db
    )

    print(f"âœ“ ä»£ç å·²æ›´æ–°åˆ°ç‰ˆæœ¬ {updated_code.current_version}")

    # äº¤äº’å¼ç”Ÿæˆï¼ˆå¤šè½®è¿­ä»£ï¼‰
    print("\nå¼€å§‹äº¤äº’å¼ä»£ç ç”Ÿæˆ...")
    result = await generator.generate_code_with_interaction(
        research_gap=gap,
        max_iterations=3
    )

    print(f"âœ“ äº¤äº’å¼ç”Ÿæˆå®Œæˆ:")
    print(f"  æœ€ç»ˆç‰ˆæœ¬: {result['total_iterations']}")
    print(f"  è´¨é‡è¯„åˆ†: {result['final_code']['quality_score']:.2f}")

# è¿è¡Œ
asyncio.run(code_generation_example())
```

### ç¤ºä¾‹5: çŸ¥è¯†å›¾è°±æŸ¥è¯¢

```python
from src.db_manager import DatabaseManager

def knowledge_graph_example():
    """çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç¤ºä¾‹"""

    db = DatabaseManager()

    # 1. åˆ›å»ºè®ºæ–‡å…³ç³»
    relation_data = {
        'source_id': 1,  # Transformerè®ºæ–‡
        'target_id': 2,  # BERTè®ºæ–‡
        'relation_type': 'extends',  # BERTæ‰©å±•äº†Transformer
        'strength': 0.9,
        'evidence': 'BERTä½¿ç”¨Transformeræ¶æ„ä½œä¸ºç¼–ç å™¨'
    }

    relation = db.create_relation(relation_data)
    print(f"âœ“ åˆ›å»ºå…³ç³»: {relation.relation_type}")

    # 2. è·å–è®ºæ–‡çš„æ‰€æœ‰å…³ç³»
    relations = db.get_relations(paper_id=1)
    print(f"\nè®ºæ–‡1çš„å…³ç³»ç½‘ç»œ:")
    for rel in relations:
        print(f"  {rel.source_id} --[{rel.relation_type}]--> {rel.target_id}")

    # 3. è·å–çŸ¥è¯†å›¾è°±æ•°æ®
    graph = db.get_paper_graph(paper_ids=[1, 2, 3])
    print(f"\nçŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
    print(f"  èŠ‚ç‚¹æ•°: {len(graph['nodes'])}")
    print(f"  è¾¹æ•°: {len(graph['edges'])}")

    # 4. å¯è§†åŒ–ï¼ˆä½¿ç”¨matplotlibï¼‰
    import matplotlib.pyplot as plt
    import networkx as nx

    G = nx.DiGraph()
    for node_id, node_data in graph['nodes'].items():
        G.add_node(node_id, label=node_data['title'][:30])

    for edge in graph['edges']:
        G.add_edge(
            edge['source'],
            edge['target'],
            relation_type=edge['type']
        )

    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue')
    plt.title("è®ºæ–‡çŸ¥è¯†å›¾è°±")
    plt.savefig('knowledge_graph.png')
    print("âœ“ çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ° knowledge_graph.png")

# è¿è¡Œ
knowledge_graph_example()
```

### ç¤ºä¾‹6: é«˜çº§æŸ¥è¯¢ä¸ç»Ÿè®¡

```python
from src.db_manager import DatabaseManager

def advanced_queries_example():
    """é«˜çº§æŸ¥è¯¢ç¤ºä¾‹"""

    db = DatabaseManager()

    # 1. è·å–æ•°æ®åº“ç»Ÿè®¡
    stats = db.get_statistics()
    print(f"\næ•°æ®åº“ç»Ÿè®¡:")
    print(f"  è®ºæ–‡æ€»æ•°: {stats['total_papers']}")
    print(f"  ä½œè€…æ€»æ•°: {stats['total_authors']}")
    print(f"  åˆ†æå®Œæˆ: {stats['completed_analyses']}")
    print(f"  ç ”ç©¶ç©ºç™½: {stats['total_gaps']}")
    print(f"  ç”Ÿæˆä»£ç : {stats['total_generated_code']}")

    # 2. å¤æ‚æŸ¥è¯¢ï¼šè·å–2020-2023å¹´çš„é¡¶ä¼šè®ºæ–‡
    papers = db.get_papers(
        year_from=2020,
        year_to=2023,
        venue='NeurIPS',  # æˆ– 'ICML', 'CVPR'ç­‰
        limit=20
    )

    print(f"\n2020-2023å¹´NeurIPSè®ºæ–‡: {len(papers)} ç¯‡")
    for paper in papers[:5]:
        print(f"  - {paper.title} ({paper.year})")

    # 3. è·å–é«˜ä¼˜å…ˆçº§ç ”ç©¶ç©ºç™½
    priority_gaps = db.get_priority_gaps(limit=10)
    print(f"\né«˜ä¼˜å…ˆçº§ç ”ç©¶ç©ºç™½:")
    for gap in priority_gaps:
        print(f"  - {gap.description[:60]}...")
        print(f"    é‡è¦æ€§: {gap.importance} | éš¾åº¦: {gap.difficulty}")

    # 4. è·å–è®ºæ–‡çš„å®Œæ•´åˆ†æå†å²
    paper_id = 1
    analyses = db.get_analyses_by_paper(paper_id)
    print(f"\nè®ºæ–‡{paper_id}çš„åˆ†æå†å²:")
    for analysis in analyses:
        print(f"  - {analysis.created_at}: {analysis.status}")
        print(f"    ä»»åŠ¡è€—æ—¶: {analysis.total_time}ç§’")
        print(f"    LLMè°ƒç”¨: {analysis.llm_calls}æ¬¡")

# è¿è¡Œ
advanced_queries_example()
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰å·¥ä½œæµ

```python
async def custom_workflow():
    """è‡ªå®šä¹‰å·¥ä½œæµ"""

    db = DatabaseManager()
    workflow = AsyncWorkflowEngine(db_manager=db)

    # åªåˆ†æï¼Œä¸ç”Ÿæˆä»£ç 
    result = await workflow.execute_paper_workflow(
        pdf_path='paper.pdf',
        tasks=['summary', 'keypoints'],  # åªæ‰§è¡Œè¿™ä¸¤ä¸ªä»»åŠ¡
        auto_generate_code=False  # ä¸è‡ªåŠ¨ç”Ÿæˆä»£ç 
    )

    return result
```

### 2. æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
# é«˜æ€§èƒ½é…ç½®
workflow = AsyncWorkflowEngine(
    db_manager=db,
    llm_config={
        'model': 'glm-4-plus',
        'max_concurrent': 10,  # å¢åŠ å¹¶å‘æ•°
        'request_timeout': 120  # å¢åŠ è¶…æ—¶æ—¶é—´
    }
)

# æ‰¹é‡å¤„ç†100ç¯‡è®ºæ–‡
results = await workflow.batch_process_papers(
    pdf_paths=list_of_100_papers,
    tasks=['summary', 'keypoints']  # å‡å°‘ä»»åŠ¡ä»¥åŠ å¿«é€Ÿåº¦
)
```

### 3. é”™è¯¯å¤„ç†ä¸é‡è¯•

```python
async def robust_workflow():
    """å®¹é”™å·¥ä½œæµ"""

    db = DatabaseManager()
    workflow = AsyncWorkflowEngine(db_manager=db)

    try:
        result = await workflow.execute_paper_workflow(
            pdf_path='paper.pdf',
            tasks=['summary', 'keypoints', 'gaps', 'code']
        )

        if result['status'] == 'completed':
            print("âœ“ åˆ†ææˆåŠŸ")
        else:
            print(f"âœ— åˆ†æå¤±è´¥: {result.get('error')}")
            # é‡è¯•é€»è¾‘
            # ...

    except Exception as e:
        print(f"âœ— å·¥ä½œæµå¼‚å¸¸: {e}")
        # ä¿å­˜é”™è¯¯æ—¥å¿—
        # ...
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### v3.0 vs v4.0

| æŒ‡æ ‡ | v3.0 | v4.0 | æå‡ |
|-----|------|------|------|
| å•ç¯‡åˆ†ææ—¶é—´ | 60ç§’ | 10ç§’ | **6x** |
| å¹¶å‘èƒ½åŠ› | 1ç¯‡ | 100ç¯‡ | **100x** |
| æ‰¹é‡å¤„ç† | ä¸æ”¯æŒ | æ”¯æŒ | **æ–°åŠŸèƒ½** |
| æ•°æ®æŒä¹…åŒ– | æ–‡ä»¶ | æ•°æ®åº“ | **è´¨å˜** |
| ä»£ç ç”Ÿæˆ | ä¸æ”¯æŒ | æ”¯æŒ | **æ–°åŠŸèƒ½** |
| çŸ¥è¯†å›¾è°± | ä¸æ”¯æŒ | æ”¯æŒ | **æ–°åŠŸèƒ½** |

---

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: æ–‡çŒ®ç»¼è¿°è‡ªåŠ¨åŒ–

```python
async def literature_review_auto():
    """è‡ªåŠ¨åŒ–æ–‡çŒ®ç»¼è¿°"""

    # 1. æ‰¹é‡åˆ†æé¢†åŸŸå†…çš„50ç¯‡è®ºæ–‡
    papers = glob.glob('nlp_papers/*.pdf')
    summary = await workflow.batch_process_papers(
        pdf_paths=papers,
        tasks=['summary', 'keypoints', 'gaps', 'graph']
    )

    # 2. æ„å»ºçŸ¥è¯†å›¾è°±
    graph = db.get_paper_graph()
    # å¯è§†åŒ–ç ”ç©¶æ–¹å‘

    # 3. æå–ç ”ç©¶ç©ºç™½
    priority_gaps = db.get_priority_gaps(limit=20)
    # æŒ‰é‡è¦æ€§æ’åº

    # 4. ç”Ÿæˆç»¼è¿°æŠ¥å‘Š
    review = {
        'total_papers': summary['total'],
        'main_directions': extract_directions(graph),
        'research_gaps': priority_gaps,
        'future_work': suggest_future_work(priority_gaps)
    }

    return review
```

### æ¡ˆä¾‹2: ä»è®ºæ–‡åˆ°ä»£ç 

```python
async def from_paper_to_code():
    """å®Œæ•´çš„è®ºæ–‡åˆ°ä»£ç æµç¨‹"""

    # 1. ä¸Šä¼ å¹¶åˆ†æè®ºæ–‡
    result = await workflow.execute_paper_workflow(
        pdf_path='new_method.pdf',
        auto_generate_code=True
    )

    # 2. æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 
    code_id = result['generated_code_id']
    code = db.get_code(code_id)

    print(f"ç”Ÿæˆçš„ä»£ç :\n{code.code}")

    # 3. ç”¨æˆ·ä¿®æ”¹
    updated = await code_generator.modify_code_async(
        code_id=code_id,
        user_prompt="è¯·æ·»åŠ GPUæ”¯æŒå’Œæ‰¹å¤„ç†åŠŸèƒ½"
    )

    # 4. è¿è¡Œä»£ç 
    exec(updated.code)  # æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­éœ€è¦æ²™ç®±ç¯å¢ƒ

    # 5. ä¿å­˜å®éªŒç»“æœ
    experiment = {
        'code_id': updated.id,
        'config': {...},
        'results': {...},
        'metrics': {...}
    }
    db.create_experiment(experiment)
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ•°æ®åº“ç®¡ç†
- å®šæœŸå¤‡ä»½æ•°æ®åº“
- ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
- ç›‘æ§æ•°æ®åº“å¤§å°

### 2. LLMè°ƒç”¨
- ä½¿ç”¨åˆé€‚çš„æ¨¡å‹ï¼ˆplus vs airï¼‰
- æ§åˆ¶å¹¶å‘æ•°é¿å…é™æµ
- å®ç°é‡è¯•æœºåˆ¶

### 3. ä»£ç è´¨é‡
- å§‹ç»ˆåœ¨æ²™ç®±ä¸­è¿è¡Œç”Ÿæˆçš„ä»£ç 
- å®¡æŸ¥AIç”Ÿæˆçš„ä»£ç 
- æ·»åŠ è‡ªå·±çš„æµ‹è¯•

### 4. æ€§èƒ½ä¼˜åŒ–
- æ‰¹é‡å¤„ç†æ—¶ä½¿ç”¨å¼‚æ­¥
- å¯ç”¨ç¼“å­˜é¿å…é‡å¤åˆ†æ
- ç›‘æ§APIè°ƒç”¨æˆæœ¬

---

## ğŸ“š æ›´å¤šç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šå®Œæ•´ç¤ºä¾‹ï¼š
- `basic_usage.py` - åŸºç¡€ç”¨æ³•
- `batch_processing.py` - æ‰¹é‡å¤„ç†
- `code_generation.py` - ä»£ç ç”Ÿæˆ
- `knowledge_graph.py` - çŸ¥è¯†å›¾è°±
- `advanced_queries.py` - é«˜çº§æŸ¥è¯¢

---

**ç‰ˆæœ¬**: v4.0 Academician Edition
**æ›´æ–°æ—¥æœŸ**: 2025å¹´

**å¼€å§‹æ‚¨çš„æ™ºèƒ½ç§‘ç ”ä¹‹æ—…ï¼** ğŸš€ğŸ“ğŸ“„
