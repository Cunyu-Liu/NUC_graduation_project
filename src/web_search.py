"""è”ç½‘æœç´¢æ¨¡å— - æ”¯æŒå¤šç§æœç´¢å¼•æ“
æä¾›å…è´¹çš„ DuckDuckGo æœç´¢ï¼Œæ— éœ€ API key
"""
import os
import re
import json
import time
import urllib.request
import urllib.parse
import urllib.error
import ssl
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    title: str
    url: str
    snippet: str
    source: str = "web"
    timestamp: Optional[str] = None


class WebSearchEngine:
    """ç½‘é¡µæœç´¢å¼•æ“"""
    
    def __init__(self):
        self.last_search_time = 0
        self.min_interval = 1.0  # æœ€å°æœç´¢é—´éš”ï¼ˆç§’ï¼‰
        # åˆ›å»º SSL ä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥è¯ä¹¦éªŒè¯ï¼ˆè§£å†³æŸäº›ç¯å¢ƒçš„ SSL é—®é¢˜ï¼‰
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
    
    def _rate_limit(self):
        """é€Ÿç‡é™åˆ¶"""
        elapsed = time.time() - self.last_search_time
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_search_time = time.time()
    
    def search_duckduckgo(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """
        ä½¿ç”¨ DuckDuckGo æœç´¢ï¼ˆå…è´¹ï¼Œæ— éœ€ API keyï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            self._rate_limit()
            
            # DuckDuckGo HTML ç‰ˆæœ¬æœç´¢
            encoded_query = urllib.parse.quote_plus(query)
            
            # å°è¯•å¤šä¸ª DuckDuckGo åŸŸå
            urls = [
                f"https://html.duckduckgo.com/html/?q={encoded_query}",
                f"https://duckduckgo.com/html/?q={encoded_query}",
            ]
            
            html = None
            for url in urls:
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1'
                    }
                    
                    req = urllib.request.Request(url, headers=headers)
                    
                    print(f"[DEBUG] å‘é€DuckDuckGoæœç´¢è¯·æ±‚: {url[:80]}...")
                    
                    with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as response:
                        # å¤„ç† gzip å‹ç¼©
                        import gzip
                        if response.headers.get('Content-Encoding') == 'gzip':
                            html = gzip.decompress(response.read()).decode('utf-8', errors='ignore')
                        else:
                            html = response.read().decode('utf-8', errors='ignore')
                        
                        if html and len(html) > 100:
                            print(f"[DEBUG] æˆåŠŸè·å–å“åº”ï¼Œé•¿åº¦: {len(html)}")
                            break
                            
                except Exception as e:
                    print(f"[DEBUG] URL {url} å¤±è´¥: {e}")
                    continue
            
            if not html:
                print("[DEBUG] æ‰€æœ‰ DuckDuckGo URL éƒ½å¤±è´¥")
                return []
            
            results = []
            
            # å°è¯•å¤šç§è§£ææ¨¡å¼
            # æ¨¡å¼1: æ–°ç‰ˆ DuckDuckGo æ ¼å¼
            result_blocks = re.findall(
                r'<div[^>]*class="[^"]*result[^"]*"[^>]*>.*?<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>.*?<a[^>]*class="[^"]*result__snippet[^"]*"[^>]*>(.*?)</a>.*?</div>',
                html, re.DOTALL | re.IGNORECASE
            )
            
            if not result_blocks:
                # æ¨¡å¼2: æ›´é€šç”¨çš„ç»“æœåŒ¹é…
                result_blocks = re.findall(
                    r'<a[^>]*class="result__a"[^>]*href="([^"]+)"[^>]*>(.*?)</a>.*?<a[^>]*class="result__snippet"[^>]*>(.*?)</a>',
                    html, re.DOTALL | re.IGNORECASE
                )
            
            if not result_blocks:
                # æ¨¡å¼3: æ›´å®½æ¾çš„åŒ¹é…
                result_blocks = re.findall(
                    r'<h[^>]*>.*?<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>.*?</h[^>]*>.*?<p[^>]*>(.*?)</p>',
                    html, re.DOTALL | re.IGNORECASE
                )
            
            if not result_blocks:
                # æ¨¡å¼4: æç®€åŒ¹é… - ç›´æ¥åŒ¹é…æ‰€æœ‰é“¾æ¥
                all_links = re.findall(r'<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>', html, re.DOTALL | re.IGNORECASE)
                print(f"[DEBUG] æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
                # è¿‡æ»¤å‡ºçœ‹èµ·æ¥åƒæ˜¯æœç´¢ç»“æœçš„é“¾æ¥
                for href, title in all_links:
                    if href.startswith('http') and not 'duckduckgo.com' in href:
                        clean_title = re.sub(r'<[^>]+>', '', title).strip()
                        if len(clean_title) > 5:
                            results.append(SearchResult(
                                title=clean_title,
                                url=href,
                                snippet="",
                                source="DuckDuckGo"
                            ))
                            if len(results) >= max_results:
                                break
            
            print(f"[DEBUG] è§£æåˆ° {len(result_blocks)} ä¸ªç»“æœå—")
            
            for href, title, snippet in result_blocks[:max_results]:
                # æ¸…ç† HTML æ ‡ç­¾
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                clean_snippet = re.sub(r'<[^>]+>', '', snippet).strip()
                
                # å¤„ç†é‡å®šå‘ URL
                if href.startswith('/l/'):
                    # æå–å®é™… URL
                    match = re.search(r'uddg=([^&]+)', href)
                    if match:
                        href = urllib.parse.unquote(match.group(1))
                elif href.startswith('//'):
                    href = 'https:' + href
                
                # è¿‡æ»¤æ— æ•ˆé“¾æ¥
                if not href.startswith('http'):
                    continue
                
                # è¿‡æ»¤ DuckDuckGo è‡ªå·±çš„é“¾æ¥
                if 'duckduckgo.com' in href:
                    continue
                    
                if clean_title and href and len(clean_title) > 3:
                    results.append(SearchResult(
                        title=clean_title,
                        url=href,
                        snippet=clean_snippet,
                        source="DuckDuckGo"
                    ))
            
            print(f"[DEBUG] DuckDuckGoæœç´¢å®Œæˆ: è¿”å›{len(results)}æ¡ç»“æœ")
            return results
            
        except Exception as e:
            print(f"âš ï¸ DuckDuckGo æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def search_bing(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """
        ä½¿ç”¨ Bing æœç´¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        éœ€è¦é…ç½® BING_API_KEY ç¯å¢ƒå˜é‡
        """
        api_key = os.getenv('BING_API_KEY')
        if not api_key:
            return []
        
        try:
            self._rate_limit()
            
            url = "https://api.bing.microsoft.com/v7.0/search"
            headers = {"Ocp-Apim-Subscription-Key": api_key}
            params = {"q": query, "count": max_results, "textDecorations": False}
            
            req = urllib.request.Request(
                f"{url}?{urllib.parse.urlencode(params)}",
                headers=headers
            )
            
            with urllib.request.urlopen(req, timeout=10, context=self.ssl_context) as response:
                data = json.loads(response.read().decode('utf-8'))
            
            results = []
            for item in data.get('webPages', {}).get('value', []):
                results.append(SearchResult(
                    title=item.get('name', ''),
                    url=item.get('url', ''),
                    snippet=item.get('snippet', ''),
                    source="Bing"
                ))
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ Bing æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_serpapi(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """
        ä½¿ç”¨ SerpAPI è¿›è¡Œ Google æœç´¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        éœ€è¦é…ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡
        """
        api_key = os.getenv('SERPAPI_KEY')
        if not api_key:
            return []
        
        try:
            self._rate_limit()
            
            params = {
                "engine": "google",
                "q": query,
                "api_key": api_key,
                "num": max_results
            }
            
            url = f"https://serpapi.com/search?{urllib.parse.urlencode(params)}"
            
            req = urllib.request.Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as response:
                data = json.loads(response.read().decode('utf-8'))
            
            results = []
            for item in data.get('organic_results', []):
                results.append(SearchResult(
                    title=item.get('title', ''),
                    url=item.get('link', ''),
                    snippet=item.get('snippet', ''),
                    source="Google (via SerpAPI)"
                ))
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ SerpAPI æœç´¢å¤±è´¥: {e}")
            return []
    
    def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """
        ç»¼åˆæœç´¢ï¼šå°è¯•å¤šç§æœç´¢å¼•æ“
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {query[:50]}...")
        
        all_results = []
        
        # ä¼˜å…ˆä½¿ç”¨ DuckDuckGoï¼ˆå…è´¹ï¼‰
        ddgs_results = self.search_duckduckgo(query, max_results)
        if ddgs_results:
            all_results.extend(ddgs_results)
            print(f"âœ… DuckDuckGo æ‰¾åˆ° {len(ddgs_results)} æ¡ç»“æœ")
        
        # å¦‚æœ DuckDuckGo ç»“æœä¸è¶³ï¼Œå°è¯• SerpAPI
        if len(all_results) < max_results:
            remaining = max_results - len(all_results)
            serp_results = self.search_serpapi(query, remaining)
            if serp_results:
                # å»é‡
                existing_urls = {r.url for r in all_results}
                for r in serp_results:
                    if r.url not in existing_urls:
                        all_results.append(r)
                        existing_urls.add(r.url)
                print(f"âœ… SerpAPI è¡¥å…… {len(serp_results)} æ¡ç»“æœ")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯• Bing
        if len(all_results) < max_results:
            remaining = max_results - len(all_results)
            bing_results = self.search_bing(query, remaining)
            if bing_results:
                existing_urls = {r.url for r in all_results}
                for r in bing_results:
                    if r.url not in existing_urls:
                        all_results.append(r)
                        existing_urls.add(r.url)
                print(f"âœ… Bing è¡¥å…… {len(bing_results)} æ¡ç»“æœ")
        
        # å¦‚æœæ‰€æœ‰æœç´¢éƒ½å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if not all_results and os.getenv('WEB_SEARCH_FALLBACK', 'false').lower() == 'true':
            print("âš ï¸ æ‰€æœ‰æœç´¢æºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æç¤º")
            all_results = [
                SearchResult(
                    title="æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                    url="https://www.google.com/search?q=" + urllib.parse.quote_plus(query),
                    snippet=f"æ— æ³•è·å–å®æ—¶æœç´¢ç»“æœã€‚è¯·ç›´æ¥è®¿é—® Google æœç´¢: {query[:50]}...",
                    source="Fallback"
                )
            ]
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_results)} æ¡ç»“æœ")
        return all_results[:max_results]
    
    def format_results_for_llm(self, results: List[SearchResult]) -> str:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸º LLM å¯ç”¨çš„ä¸Šä¸‹æ–‡
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not results:
            return ""
        
        lines = ["ã€ç½‘ç»œæœç´¢ç»“æœã€‘"]
        lines.append(f"ï¼ˆå…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœï¼‰\n")
        
        for i, r in enumerate(results, 1):
            lines.append(f"{i}. {r.title}")
            lines.append(f"   æ¥æº: {r.source}")
            lines.append(f"   é“¾æ¥: {r.url}")
            if r.snippet:
                lines.append(f"   æ‘˜è¦: {r.snippet}")
            lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(lines)


# å…¨å±€æœç´¢å¼•æ“å®ä¾‹
_search_engine = None

def get_search_engine() -> WebSearchEngine:
    """è·å–æœç´¢å¼•æ“å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _search_engine
    if _search_engine is None:
        _search_engine = WebSearchEngine()
    return _search_engine


def web_search(query: str, max_results: int = 5) -> List[SearchResult]:
    """
    ä¾¿æ·çš„æœç´¢å‡½æ•°
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_results: è¿”å›ç»“æœæ•°é‡
        
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
        
    Example:
        >>> results = web_search("è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æœ€æ–°è¿›å±•", max_results=3)
        >>> for r in results:
        ...     print(f"{r.title}: {r.url}")
    """
    engine = get_search_engine()
    return engine.search(query, max_results)


if __name__ == "__main__":
    # æµ‹è¯•
    print("=" * 60)
    print("æµ‹è¯•è”ç½‘æœç´¢åŠŸèƒ½")
    print("=" * 60)
    
    test_queries = [
        "Python programming language",
        "æ·±åº¦å­¦ä¹ æœ€æ–°è¿›å±•",
        "transformer architecture"
    ]
    
    engine = get_search_engine()
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: {query}")
        print("-" * 40)
        results = engine.search(query, max_results=3)
        for r in results:
            print(f"\næ ‡é¢˜: {r.title}")
            print(f"é“¾æ¥: {r.url}")
            print(f"æ‘˜è¦: {r.snippet[:100]}..." if len(r.snippet) > 100 else f"æ‘˜è¦: {r.snippet}")
            print(f"æ¥æº: {r.source}")
