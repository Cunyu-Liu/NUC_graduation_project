"""Milvus å‘é‡æ•°æ®åº“é›†æˆæ¨¡å— - v4.2é™¢å£«ç‰ˆ
æ”¯æŒè®ºæ–‡ embedding å­˜å‚¨ã€è¯­ä¹‰æœç´¢ã€å‘é‡èšç±»
"""
import os
import json
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

# å°è¯•å¯¼å…¥ Milvus å®¢æˆ·ç«¯
try:
    from pymilvus import (
        connections, Collection, CollectionSchema, FieldSchema, DataType,
        utility, IndexType, MetricType
    )
    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    print("âš ï¸  pymilvus æœªå®‰è£…ï¼Œå‘é‡æ•°æ®åº“åŠŸèƒ½å°†å—é™")

# å°è¯•å¯¼å…¥ sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  sentence-transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ API ç”Ÿæˆ embedding")


@dataclass
class VectorSearchResult:
    """å‘é‡æœç´¢ç»“æœ"""
    paper_id: int
    distance: float
    title: str
    abstract: str
    year: Optional[int] = None
    venue: Optional[str] = None


@dataclass
class PaperEmbedding:
    """è®ºæ–‡å‘é‡è¡¨ç¤º"""
    paper_id: int
    title: str
    abstract: str
    embedding: np.ndarray
    keywords: List[str]
    year: Optional[int] = None
    venue: Optional[str] = None
    authors: List[str] = None


class MilvusVectorStore:
    """Milvus å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    # é›†åˆé…ç½®
    COLLECTION_NAME = "paper_embeddings"
    DIM = 1024  # å‘é‡ç»´åº¦ (BGE-large-zh-v1.5 ä¸º 1024)
    INDEX_TYPE = "IVF_FLAT"  # ç´¢å¼•ç±»å‹
    METRIC_TYPE = "L2"  # è·ç¦»åº¦é‡
    
    def __init__(self, 
                 host: str = "localhost", 
                 port: str = "19530",
                 db_manager=None):
        """
        åˆå§‹åŒ– Milvus å‘é‡å­˜å‚¨
        
        Args:
            host: Milvus æœåŠ¡å™¨åœ°å€
            port: Milvus ç«¯å£
            db_manager: æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
        """
        if not MILVUS_AVAILABLE:
            raise ImportError("pymilvus æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å‘é‡æ•°æ®åº“åŠŸèƒ½")
        
        self.host = host or os.getenv('MILVUS_HOST', 'localhost')
        self.port = port or os.getenv('MILVUS_PORT', '19530')
        self.db_manager = db_manager
        self.collection = None
        self.embedding_model = None
        
        # åˆå§‹åŒ– embedding æ¨¡å‹
        self._init_embedding_model()
        
        # è¿æ¥åˆ° Milvus
        self._connect()
        
        # åˆå§‹åŒ–é›†åˆ
        self._init_collection()
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ– embedding æ¨¡å‹"""
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                # ä½¿ç”¨ BGE-large-zh-v1.5ï¼Œä¸­æ–‡æ•ˆæœä¼˜ç§€
                model_name = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-large-zh-v1.5')
                print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ embedding æ¨¡å‹: {model_name}")
                self.embedding_model = SentenceTransformer(model_name)
                self.DIM = self.embedding_model.get_sentence_embedding_dimension()
                print(f"âœ… Embedding æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.DIM}")
            except Exception as e:
                print(f"âš ï¸  Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.embedding_model = None
        else:
            # ä½¿ç”¨ LLM API ç”Ÿæˆ embedding
            self.embedding_model = None
            print("âš ï¸  å°†ä½¿ç”¨ API ç”Ÿæˆ embedding")
    
    def _connect(self):
        """è¿æ¥åˆ° Milvus æœåŠ¡å™¨"""
        try:
            connections.connect(
                alias="default",
                host=self.host,
                port=self.port
            )
            print(f"âœ… å·²è¿æ¥åˆ° Milvus: {self.host}:{self.port}")
        except Exception as e:
            print(f"âŒ è¿æ¥ Milvus å¤±è´¥: {e}")
            raise
    
    def _init_collection(self):
        """åˆå§‹åŒ–é›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰"""
        if not utility.has_collection(self.COLLECTION_NAME):
            print(f"ğŸ“¦ åˆ›å»ºæ–°é›†åˆ: {self.COLLECTION_NAME}")
            self._create_collection()
        else:
            print(f"ğŸ“¦ ä½¿ç”¨å·²æœ‰é›†åˆ: {self.COLLECTION_NAME}")
            self.collection = Collection(self.COLLECTION_NAME)
    
    def _create_collection(self):
        """åˆ›å»ºå‘é‡é›†åˆ"""
        # å®šä¹‰å­—æ®µ
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="paper_id", dtype=DataType.INT64),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="abstract", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.DIM),
            FieldSchema(name="keywords", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="authors", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="year", dtype=DataType.INT32),
            FieldSchema(name="venue", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="created_at", dtype=DataType.INT64),
        ]
        
        # åˆ›å»º schema
        schema = CollectionSchema(fields, description="è®ºæ–‡å‘é‡åµŒå…¥é›†åˆ")
        
        # åˆ›å»ºé›†åˆ
        self.collection = Collection(self.COLLECTION_NAME, schema)
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "metric_type": self.METRIC_TYPE,
            "index_type": self.INDEX_TYPE,
            "params": {"nlist": 128}
        }
        self.collection.create_index(field_name="embedding", index_params=index_params)
        print(f"âœ… é›†åˆåˆ›å»ºæˆåŠŸï¼Œç´¢å¼•ç±»å‹: {self.INDEX_TYPE}")
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        ç”Ÿæˆæ–‡æœ¬çš„ embedding
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            embedding å‘é‡
        """
        if self.embedding_model:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            embedding = self.embedding_model.encode(text, normalize_embeddings=True)
            return embedding
        else:
            # ä½¿ç”¨ API ç”Ÿæˆ embedding (éœ€è¦å®ç°)
            raise NotImplementedError("API embedding ç”ŸæˆåŠŸèƒ½å¾…å®ç°")
    
    def add_paper(self, paper: PaperEmbedding) -> bool:
        """
        æ·»åŠ å•ç¯‡è®ºæ–‡åˆ°å‘é‡å­˜å‚¨
        
        Args:
            paper: è®ºæ–‡å‘é‡è¡¨ç¤º
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            entities = [
                [paper.paper_id],
                [paper.title],
                [paper.abstract],
                [paper.embedding.tolist()],
                [json.dumps(paper.keywords)],
                [json.dumps(paper.authors or [])],
                [paper.year or 0],
                [paper.venue or ""],
                [int(datetime.now().timestamp())]
            ]
            
            self.collection.insert(entities)
            self.collection.flush()
            return True
        except Exception as e:
            print(f"âŒ æ·»åŠ è®ºæ–‡å¤±è´¥: {e}")
            return False
    
    def add_papers_batch(self, papers: List[PaperEmbedding], batch_size: int = 100) -> Dict[str, Any]:
        """
        æ‰¹é‡æ·»åŠ è®ºæ–‡åˆ°å‘é‡å­˜å‚¨
        
        Args:
            papers: è®ºæ–‡å‘é‡è¡¨ç¤ºåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ“ä½œç»“æœç»Ÿè®¡
        """
        results = {"success": 0, "failed": 0, "errors": []}
        
        for i in range(0, len(papers), batch_size):
            batch = papers[i:i+batch_size]
            try:
                entities = [
                    [p.paper_id for p in batch],
                    [p.title for p in batch],
                    [p.abstract for p in batch],
                    [p.embedding.tolist() for p in batch],
                    [json.dumps(p.keywords) for p in batch],
                    [json.dumps(p.authors or []) for p in batch],
                    [p.year or 0 for p in batch],
                    [p.venue or "" for p in batch],
                    [int(datetime.now().timestamp())] * len(batch)
                ]
                
                self.collection.insert(entities)
                results["success"] += len(batch)
            except Exception as e:
                results["failed"] += len(batch)
                results["errors"].append(str(e))
                print(f"âŒ æ‰¹é‡æ·»åŠ å¤±è´¥: {e}")
        
        self.collection.flush()
        return results
    
    def search(self, 
               query: str, 
               top_k: int = 10,
               paper_ids: Optional[List[int]] = None) -> List[VectorSearchResult]:
        """
        è¯­ä¹‰æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            paper_ids: å¯é€‰çš„è®ºæ–‡IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # åŠ è½½é›†åˆåˆ°å†…å­˜
        self.collection.load()
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.generate_embedding(query)
        
        # æœç´¢å‚æ•°
        search_params = {"metric_type": self.METRIC_TYPE, "params": {"nprobe": 10}}
        
        # æ‰§è¡Œæœç´¢
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=f"paper_id in {paper_ids}" if paper_ids else None,
            output_fields=["paper_id", "title", "abstract", "year", "venue"]
        )
        
        # è§£æç»“æœ
        search_results = []
        for hits in results:
            for hit in hits:
                search_results.append(VectorSearchResult(
                    paper_id=hit.entity.get("paper_id"),
                    distance=hit.distance,
                    title=hit.entity.get("title"),
                    abstract=hit.entity.get("abstract"),
                    year=hit.entity.get("year"),
                    venue=hit.entity.get("venue")
                ))
        
        return search_results
    
    def find_similar_papers(self, 
                           paper_id: int, 
                           top_k: int = 5) -> List[VectorSearchResult]:
        """
        æŸ¥æ‰¾ä¸æŒ‡å®šè®ºæ–‡ç›¸ä¼¼çš„è®ºæ–‡
        
        Args:
            paper_id: è®ºæ–‡ID
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼è®ºæ–‡åˆ—è¡¨
        """
        # åŠ è½½é›†åˆ
        self.collection.load()
        
        # æŸ¥è¯¢è¯¥è®ºæ–‡çš„ embedding
        results = self.collection.query(
            expr=f"paper_id == {paper_id}",
            output_fields=["embedding", "title", "abstract"]
        )
        
        if not results:
            return []
        
        paper_embedding = results[0]["embedding"]
        
        # æœç´¢ç›¸ä¼¼è®ºæ–‡
        search_params = {"metric_type": self.METRIC_TYPE, "params": {"nprobe": 10}}
        
        similar_results = self.collection.search(
            data=[paper_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k + 1,  # +1 å› ä¸ºè¦æ’é™¤è‡ªå·±
            expr=f"paper_id != {paper_id}",
            output_fields=["paper_id", "title", "abstract", "year", "venue"]
        )
        
        # è§£æç»“æœ
        similar_papers = []
        for hits in similar_results:
            for hit in hits[:top_k]:  # åªå–å‰ top_k ä¸ª
                similar_papers.append(VectorSearchResult(
                    paper_id=hit.entity.get("paper_id"),
                    distance=hit.distance,
                    title=hit.entity.get("title"),
                    abstract=hit.entity.get("abstract"),
                    year=hit.entity.get("year"),
                    venue=hit.entity.get("venue")
                ))
        
        return similar_papers
    
    def cluster_papers(self, 
                      paper_ids: Optional[List[int]] = None,
                      n_clusters: int = 5) -> Dict[str, Any]:
        """
        åŸºäºå‘é‡çš„è®ºæ–‡èšç±»
        
        Args:
            paper_ids: è¦èšç±»çš„è®ºæ–‡IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            n_clusters: èšç±»æ•°é‡
            
        Returns:
            èšç±»ç»“æœ
        """
        from sklearn.cluster import KMeans
        
        # åŠ è½½é›†åˆ
        self.collection.load()
        
        # è·å–æ‰€æœ‰è®ºæ–‡çš„å‘é‡
        if paper_ids:
            expr = f"paper_id in {paper_ids}"
        else:
            expr = None
        
        results = self.collection.query(
            expr=expr,
            output_fields=["paper_id", "title", "abstract", "embedding", "year", "venue"],
            limit=10000
        )
        
        if len(results) < n_clusters:
            return {"error": f"è®ºæ–‡æ•°é‡({len(results)})å°‘äºèšç±»æ•°é‡({n_clusters})"}
        
        # æå–å‘é‡
        embeddings = np.array([r["embedding"] for r in results])
        paper_data = [
            {
                "paper_id": r["paper_id"],
                "title": r["title"],
                "abstract": r["abstract"],
                "year": r.get("year"),
                "venue": r.get("venue")
            }
            for r in results
        ]
        
        # æ‰§è¡Œ K-Means èšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # æ•´ç†èšç±»ç»“æœ
        clusters = {i: [] for i in range(n_clusters)}
        for idx, label in enumerate(labels):
            clusters[label].append(paper_data[idx])
        
        # è®¡ç®—æ¯ä¸ªèšç±»çš„ä¸­å¿ƒè®ºæ–‡ï¼ˆç¦»è´¨å¿ƒæœ€è¿‘çš„ï¼‰
        cluster_analysis = {}
        for cluster_id, papers in clusters.items():
            if papers:
                cluster_analysis[str(cluster_id)] = {
                    "paper_count": len(papers),
                    "papers": [p["title"] for p in papers],
                    "representative_papers": papers[:3],  # å‰3ç¯‡ä½œä¸ºä»£è¡¨æ€§è®ºæ–‡
                    "years": [p["year"] for p in papers if p.get("year")]
                }
        
        return {
            "n_clusters": n_clusters,
            "total_papers": len(results),
            "method": "kmeans_vector",
            "cluster_analysis": cluster_analysis,
            "inertia": float(kmeans.inertia_)
        }
    
    def delete_paper(self, paper_id: int) -> bool:
        """
        ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤è®ºæ–‡
        
        Args:
            paper_id: è®ºæ–‡ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            self.collection.delete(expr=f"paper_id == {paper_id}")
            self.collection.flush()
            return True
        except Exception as e:
            print(f"âŒ åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.collection.load()
            count = self.collection.num_entities
            return {
                "total_papers": count,
                "collection_name": self.COLLECTION_NAME,
                "dimension": self.DIM,
                "connected": True
            }
        except Exception as e:
            return {
                "total_papers": 0,
                "collection_name": self.COLLECTION_NAME,
                "dimension": self.DIM,
                "connected": False,
                "error": str(e)
            }
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            connections.disconnect("default")
            print("âœ… Milvus è¿æ¥å·²å…³é—­")
        except:
            pass


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, db_manager=None):
        if self._initialized:
            return
        
        self._initialized = True
        self.db_manager = db_manager
        self.vector_store = None
        
        # å°è¯•åˆå§‹åŒ–
        self._try_init()
    
    def _try_init(self):
        """å°è¯•åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        if not MILVUS_AVAILABLE:
            print("âš ï¸  Milvus ä¸å¯ç”¨")
            return
        
        try:
            self.vector_store = MilvusVectorStore(db_manager=self.db_manager)
            print("âœ… å‘é‡å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_store = None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å¯ç”¨"""
        return self.vector_store is not None
    
    def sync_papers_from_db(self, paper_ids: Optional[List[int]] = None) -> Dict[str, Any]:
        """
        ä»æ•°æ®åº“åŒæ­¥è®ºæ–‡åˆ°å‘é‡å­˜å‚¨
        
        Args:
            paper_ids: è¦åŒæ­¥çš„è®ºæ–‡IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            
        Returns:
            åŒæ­¥ç»“æœ
        """
        if not self.is_available():
            return {"error": "å‘é‡å­˜å‚¨ä¸å¯ç”¨"}
        
        if not self.db_manager:
            return {"error": "æ•°æ®åº“ç®¡ç†å™¨æœªé…ç½®"}
        
        # è·å–è®ºæ–‡æ•°æ®
        if paper_ids:
            papers = []
            for pid in paper_ids:
                paper = self.db_manager.get_paper(pid)
                if paper:
                    papers.append(paper)
        else:
            papers = self.db_manager.get_all_papers(limit=10000)
        
        if not papers:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡"}
        
        # è½¬æ¢ä¸º PaperEmbedding
        paper_embeddings = []
        for paper in papers:
            try:
                # ç»„åˆæ–‡æœ¬ç”¨äºç”Ÿæˆ embedding
                text = f"{paper.get('title', '')}\n{paper.get('abstract', '')}"
                
                # è·å–å…³é”®è¯
                keywords = paper.get('keywords', [])
                if isinstance(keywords, str):
                    keywords = json.loads(keywords)
                
                # è·å–ä½œè€…
                authors = paper.get('authors', [])
                
                embedding = self.vector_store.generate_embedding(text)
                
                paper_embeddings.append(PaperEmbedding(
                    paper_id=paper['id'],
                    title=paper.get('title', ''),
                    abstract=paper.get('abstract', ''),
                    embedding=embedding,
                    keywords=keywords,
                    year=paper.get('year'),
                    venue=paper.get('venue'),
                    authors=authors
                ))
            except Exception as e:
                print(f"âš ï¸  å¤„ç†è®ºæ–‡ {paper.get('id')} å¤±è´¥: {e}")
                continue
        
        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        results = self.vector_store.add_papers_batch(paper_embeddings)
        
        return {
            "synced": results["success"],
            "failed": results["failed"],
            "total": len(papers)
        }
    
    def search(self, query: str, top_k: int = 10) -> List[VectorSearchResult]:
        """è¯­ä¹‰æœç´¢"""
        if not self.is_available():
            return []
        return self.vector_store.search(query, top_k)
    
    def find_similar(self, paper_id: int, top_k: int = 5) -> List[VectorSearchResult]:
        """æŸ¥æ‰¾ç›¸ä¼¼è®ºæ–‡"""
        if not self.is_available():
            return []
        return self.vector_store.find_similar_papers(paper_id, top_k)
    
    def cluster(self, paper_ids: Optional[List[int]] = None, n_clusters: int = 5) -> Dict[str, Any]:
        """å‘é‡èšç±»"""
        if not self.is_available():
            return {"error": "å‘é‡å­˜å‚¨ä¸å¯ç”¨"}
        return self.vector_store.cluster_papers(paper_ids, n_clusters)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_available():
            return {"available": False}
        return self.vector_store.get_stats()


# å…¨å±€å®ä¾‹
vector_store_manager = None

def get_vector_store_manager(db_manager=None) -> VectorStoreManager:
    """è·å–å‘é‡å­˜å‚¨ç®¡ç†å™¨å®ä¾‹"""
    global vector_store_manager
    if vector_store_manager is None:
        vector_store_manager = VectorStoreManager(db_manager=db_manager)
    return vector_store_manager
