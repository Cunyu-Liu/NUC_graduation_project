"""è¦ç‚¹æå–æ¨¡å— - è¯†åˆ«è®ºæ–‡æ ¸å¿ƒåˆ›æ–°ã€å®éªŒæ–¹æ³•ä¸ç»“è®º"""
import json
from pathlib import Path
from typing import Dict, List, Optional, Any
from langchain_openai import ChatOpenAI

from src.config import settings
from src.pdf_parser import ParsedPaper
from src.prompts import get_keypoint_prompt


class KeypointExtractor:
    """è®ºæ–‡è¦ç‚¹æå–å™¨"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–è¦ç‚¹æå–å™¨

        Args:
            api_key: GLM-4 APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_key = api_key or settings.glm_api_key
        self.base_url = base_url or settings.glm_base_url
        self.model = model or settings.default_model
        self.temperature = temperature if temperature is not None else settings.default_temperature
        self.max_tokens = max_tokens or settings.max_tokens

        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®GLM_API_KEYç¯å¢ƒå˜é‡")

        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )

    def extract_keypoints(
        self,
        paper: ParsedPaper,
        save: bool = True,
        output_dir: Optional[Path] = None
    ) -> Dict[str, List[str]]:
        """
        æå–è®ºæ–‡è¦ç‚¹

        Args:
            paper: è§£æåçš„è®ºæ–‡å¯¹è±¡
            save: æ˜¯å¦ä¿å­˜è¦ç‚¹åˆ°æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Dict[str, List[str]]: æå–çš„è¦ç‚¹å­—å…¸
        """
        # å‡†å¤‡å†…å®¹
        content = self._prepare_paper_content(paper)
        sections_text = "\n".join([
            f"### {name}\n{content_part[:500]}"
            for name, content_part in paper.metadata.sections.items()
        ])
        keywords_text = ", ".join(paper.metadata.keywords) if paper.metadata.keywords else "æœªæå–åˆ°å…³é”®è¯"

        # ä½¿ç”¨ä¸“ä¸šæç¤ºè¯
        prompt = get_keypoint_prompt(
            title=paper.metadata.title or paper.filename,
            abstract=paper.metadata.abstract or "æœªæå–åˆ°æ‘˜è¦",
            keywords=keywords_text,
            sections=sections_text,
            content=content
        )

        # ç”Ÿæˆè¦ç‚¹
        try:
            from langchain_core.messages import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content
            keypoints = self._parse_response(result)
        except Exception as e:
            print(f"è¦ç‚¹æå–å¤±è´¥: {e}")
            keypoints = self._get_empty_keypoints()

        # ä¿å­˜è¦ç‚¹
        if save:
            self._save_keypoints(paper, keypoints, output_dir)

        return keypoints

    def _prepare_paper_content(self, paper: ParsedPaper, max_chars: int = 8000) -> str:
        """
        å‡†å¤‡è®ºæ–‡å†…å®¹

        Args:
            paper: è®ºæ–‡å¯¹è±¡
            max_chars: æœ€å¤§å­—ç¬¦æ•°

        Returns:
            str: å¤„ç†åçš„è®ºæ–‡å†…å®¹
        """
        content_parts = []

        # æ·»åŠ æ ‡é¢˜
        if paper.metadata.title:
            content_parts.append(f"æ ‡é¢˜: {paper.metadata.title}")

        # æ·»åŠ æ‘˜è¦
        if paper.metadata.abstract:
            content_parts.append(f"æ‘˜è¦: {paper.metadata.abstract}")

        # æ·»åŠ ä¸»è¦ç« èŠ‚
        if paper.metadata.sections:
            content_parts.append("\nä¸»è¦ç« èŠ‚:")
            for section_name, section_content in paper.metadata.sections.items():
                content_parts.append(f"\n{section_name}:\n{section_content[:1500]}")

        # ç»„åˆå†…å®¹
        combined_content = "\n\n".join(content_parts)

        if len(combined_content) < max_chars:
            remaining_chars = max_chars - len(combined_content)
            full_text_sample = paper.full_text[:remaining_chars]
            combined_content += f"\n\nè®ºæ–‡æ­£æ–‡:\n{full_text_sample}"

        return combined_content[:max_chars]

    def _parse_response(self, response: str) -> Dict[str, List[str]]:
        """
        è§£æLLMå“åº”ï¼Œæå–JSONæ•°æ®

        Args:
            response: LLMå“åº”æ–‡æœ¬

        Returns:
            Dict[str, List[str]]: è§£æåçš„è¦ç‚¹å­—å…¸
        """
        # å°è¯•æå–JSONéƒ¨åˆ†
        try:
            # æŸ¥æ‰¾JSONä»£ç å—
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            elif "```" in response:
                json_start = response.find("```") + 3
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()

            # è§£æJSON
            keypoints = json.loads(json_str)

            # éªŒè¯å¹¶ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            required_fields = ["innovations", "methods", "experiments", "conclusions", "contributions", "limitations"]
            for field in required_fields:
                if field not in keypoints:
                    keypoints[field] = []
                elif not isinstance(keypoints[field], list):
                    keypoints[field] = [str(keypoints[field])]

            return keypoints

        except Exception as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response}")
            return self._get_empty_keypoints()

    def _get_empty_keypoints(self) -> Dict[str, List[str]]:
        """è¿”å›ç©ºçš„è¦ç‚¹ç»“æ„"""
        return {
            "innovations": [],
            "methods": [],
            "experiments": [],
            "conclusions": [],
            "contributions": [],
            "limitations": []
        }

    def _save_keypoints(
        self,
        paper: ParsedPaper,
        keypoints: Dict[str, List[str]],
        output_dir: Optional[Path] = None
    ):
        """
        ä¿å­˜è¦ç‚¹åˆ°æ–‡ä»¶

        Args:
            paper: è®ºæ–‡å¯¹è±¡
            keypoints: æå–çš„è¦ç‚¹
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir = output_dir or settings.keypoints_output_dir
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆå»æ‰.pdfï¼‰ä½œä¸ºè¾“å‡ºæ–‡ä»¶å
        output_filename = Path(paper.filename).stem + "_keypoints.txt"
        output_path = output_dir / output_filename

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"è®ºæ–‡æ ‡é¢˜: {paper.metadata.title or paper.filename}\n")
            f.write(f"æ–‡ä»¶å: {paper.filename}\n")
            f.write(f"é¡µæ•°: {paper.page_count}\n")
            f.write(f"\n{'='*60}\n\n")
            f.write("æ ¸å¿ƒè¦ç‚¹æŠ¥å‘Š\n\n")

            # å†™å…¥å„ä¸ªç±»åˆ«
            field_names = {
                "innovations": "ğŸ”¥ æ ¸å¿ƒåˆ›æ–°ç‚¹",
                "methods": "ğŸ”§ ä¸»è¦æ–¹æ³•ä¸æŠ€æœ¯",
                "experiments": "ğŸ§ª å®éªŒè®¾è®¡ä¸è¯„ä¼°",
                "conclusions": "ğŸ’¡ ä¸»è¦ç»“è®º",
                "contributions": "ğŸ¯ å­¦æœ¯è´¡çŒ®",
                "limitations": "âš ï¸ å±€é™æ€§"
            }

            for field, display_name in field_names.items():
                f.write(f"{display_name}\n")
                items = keypoints.get(field, [])
                if items:
                    for i, item in enumerate(items, 1):
                        f.write(f"  {i}. {item}\n")
                else:
                    f.write("  (æœªæå–åˆ°)\n")
                f.write("\n")

            f.write(f"{'='*60}\n")

    def batch_extract_keypoints(
        self,
        papers: List[ParsedPaper],
        output_dir: Optional[Path] = None
    ) -> List[Dict[str, List[str]]]:
        """
        æ‰¹é‡æå–è¦ç‚¹

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            List[Dict[str, List[str]]]: è¦ç‚¹åˆ—è¡¨
        """
        all_keypoints = []

        for i, paper in enumerate(papers, 1):
            print(f"æ­£åœ¨æå–ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡çš„è¦ç‚¹...")
            try:
                keypoints = self.extract_keypoints(paper, save=True, output_dir=output_dir)
                all_keypoints.append(keypoints)
                print(f"âœ“ å®Œæˆ: {paper.filename}")
            except Exception as e:
                print(f"âœ— å¤±è´¥: {paper.filename} - {e}")
                all_keypoints.append(self._get_empty_keypoints())

        return all_keypoints

    def generate_summary_report(
        self,
        keypoints: Dict[str, List[str]],
        paper_title: str
    ) -> str:
        """
        ç”Ÿæˆè¦ç‚¹æ‘˜è¦æŠ¥å‘Š

        Args:
            keypoints: æå–çš„è¦ç‚¹
            paper_title: è®ºæ–‡æ ‡é¢˜

        Returns:
            str: æ‘˜è¦æŠ¥å‘Š
        """
        report_lines = [
            f"# {paper_title}",
            "",
            "## æ ¸å¿ƒåˆ›æ–°ç‚¹"
        ]

        for item in keypoints.get("innovations", []):
            report_lines.append(f"- {item}")

        report_lines.extend([
            "",
            "## ä¸»è¦æ–¹æ³•"
        ])

        for item in keypoints.get("methods", []):
            report_lines.append(f"- {item}")

        report_lines.extend([
            "",
            "## ä¸»è¦ç»“è®º"
        ])

        for item in keypoints.get("conclusions", []):
            report_lines.append(f"- {item}")

        return "\n".join(report_lines)


def extract_keypoints_from_pdf(pdf_path: str) -> Dict[str, List[str]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»PDFæ–‡ä»¶æå–è¦ç‚¹

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„

    Returns:
        Dict[str, List[str]]: æå–çš„è¦ç‚¹
    """
    from src.pdf_parser import PDFParser

    # è§£æPDF
    parser = PDFParser()
    paper = parser.parse_pdf(pdf_path)

    # æå–è¦ç‚¹
    extractor = KeypointExtractor()
    keypoints = extractor.extract_keypoints(paper)

    return keypoints
