"""è¦ç‚¹æå–æ¨¡å— - v4.2 LangChainä¼˜åŒ–ç‰ˆ

ä½¿ç”¨ LangChain Prompt Templates å’Œ Pydantic Output Parser
å®ç°ç»“æ„åŒ–è¾“å‡ºï¼Œå‡å°‘è§£æé”™è¯¯
"""
import json
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.config import settings
from src.pdf_parser import ParsedPaper

# v4.2: ä½¿ç”¨æ–°çš„ LangChain è¾…åŠ©æ¨¡å—
try:
    from src.langchain_helpers import StructuredLLMHelper, get_structured_llm_helper
    from src.prompts_langchain import get_empty_keypoints
    LANGCHAIN_V2_AVAILABLE = True
except ImportError:
    LANGCHAIN_V2_AVAILABLE = False
    print("[WARNING] æ–°çš„ LangChain æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é™çº§æ¨¡å¼")

# ä¿ç•™æ—§ç‰ˆå¯¼å…¥ä»¥ç¡®ä¿å…¼å®¹æ€§
try:
    from langchain_openai import ChatOpenAI
    from src.prompts import get_keypoint_prompt
    LANGCHAIN_LEGACY_AVAILABLE = True
except ImportError:
    LANGCHAIN_LEGACY_AVAILABLE = False


class KeypointExtractor:
    """è®ºæ–‡è¦ç‚¹æå–å™¨ - v4.2 LangChainä¼˜åŒ–ç‰ˆ"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        use_langchain_v2: bool = True  # v4.2: é»˜è®¤ä½¿ç”¨æ–°çš„æ¶æ„
    ):
        """
        åˆå§‹åŒ–è¦ç‚¹æå–å™¨

        Args:
            api_key: GLM-4 APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            use_langchain_v2: æ˜¯å¦ä½¿ç”¨æ–°çš„ LangChain æ¶æ„
        """
        self.api_key = api_key or settings.glm_api_key
        self.base_url = base_url or settings.glm_base_url
        self.model = model or settings.default_model
        self.temperature = temperature if temperature is not None else settings.default_temperature
        self.max_tokens = max_tokens or settings.max_tokens
        self.use_langchain_v2 = use_langchain_v2 and LANGCHAIN_V2_AVAILABLE

        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®GLM_API_KEYç¯å¢ƒå˜é‡")

        # v4.2: åˆå§‹åŒ–æ–°çš„ LangChain helper
        if self.use_langchain_v2:
            self.helper = get_structured_llm_helper(
                api_key=self.api_key,
                base_url=self.base_url,
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            self.llm = self.helper.llm
        # å¦åˆ™ä½¿ç”¨æ—§çš„åˆå§‹åŒ–æ–¹å¼
        elif LANGCHAIN_LEGACY_AVAILABLE:
            self.llm = ChatOpenAI(
                model=self.model,
                api_key=self.api_key,
                base_url=self.base_url,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )
            self.helper = None
        else:
            raise ValueError("LangChain ä¸å¯ç”¨ä¸”æœªå¯ç”¨ v2 æ¨¡å¼")

    def extract_keypoints(
        self,
        paper: ParsedPaper,
        save: bool = True,
        output_dir: Optional[Path] = None
    ) -> Dict[str, List[str]]:
        """
        æå–è®ºæ–‡è¦ç‚¹

        Args:
            paper: è§£æåçš„è®ºæ–‡å¯¹è±¡
            save: æ˜¯å¦ä¿å­˜è¦ç‚¹åˆ°æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Dict[str, List[str]]: æå–çš„è¦ç‚¹å­—å…¸
        """
        # å‡†å¤‡å†…å®¹
        content = self._prepare_paper_content(paper)
        sections_text = "\n".join([
            f"### {name}\n{content_part[:500]}"
            for name, content_part in paper.metadata.sections.items()
        ])
        keywords_text = ", ".join(paper.metadata.keywords) if paper.metadata.keywords else "æœªæå–åˆ°å…³é”®è¯"

        # v4.2: ä½¿ç”¨æ–°çš„ helper æå–è¦ç‚¹ï¼ˆä½¿ç”¨ Pydantic Output Parserï¼‰
        if self.use_langchain_v2 and self.helper:
            keypoints = self.helper.extract_keypoints(
                title=paper.metadata.title or paper.filename,
                abstract=paper.metadata.abstract or "æœªæå–åˆ°æ‘˜è¦",
                keywords=keywords_text,
                sections=sections_text,
                content=content
            )
        else:
            # æ—§ç‰ˆå…¼å®¹æ¨¡å¼ï¼ˆæ‰‹åŠ¨è§£æ JSONï¼‰
            from langchain_core.messages import HumanMessage
            prompt = get_keypoint_prompt(
                title=paper.metadata.title or paper.filename,
                abstract=paper.metadata.abstract or "æœªæå–åˆ°æ‘˜è¦",
                keywords=keywords_text,
                sections=sections_text,
                content=content
            )
            response = self.llm.invoke([HumanMessage(content=prompt)])
            keypoints = self._parse_response(response.content)

        # ä¿å­˜è¦ç‚¹
        if save:
            self._save_keypoints(paper, keypoints, output_dir)

        return keypoints

    async def aextract_keypoints(
        self,
        paper: ParsedPaper,
        save: bool = True,
        output_dir: Optional[Path] = None
    ) -> Dict[str, List[str]]:
        """
        å¼‚æ­¥æå–è®ºæ–‡è¦ç‚¹

        Args:
            paper: è§£æåçš„è®ºæ–‡å¯¹è±¡
            save: æ˜¯å¦ä¿å­˜è¦ç‚¹åˆ°æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Dict[str, List[str]]: æå–çš„è¦ç‚¹å­—å…¸
        """
        # å‡†å¤‡å†…å®¹
        content = self._prepare_paper_content(paper)
        sections_text = "\n".join([
            f"### {name}\n{content_part[:500]}"
            for name, content_part in paper.metadata.sections.items()
        ])
        keywords_text = ", ".join(paper.metadata.keywords) if paper.metadata.keywords else "æœªæå–åˆ°å…³é”®è¯"

        # v4.2: ä½¿ç”¨æ–°çš„ helper å¼‚æ­¥æå–è¦ç‚¹
        if self.use_langchain_v2 and self.helper:
            keypoints = await self.helper.aextract_keypoints(
                title=paper.metadata.title or paper.filename,
                abstract=paper.metadata.abstract or "æœªæå–åˆ°æ‘˜è¦",
                keywords=keywords_text,
                sections=sections_text,
                content=content
            )
        else:
            # æ—§ç‰ˆå…¼å®¹æ¨¡å¼
            import asyncio
            from langchain_core.messages import HumanMessage
            prompt = get_keypoint_prompt(
                title=paper.metadata.title or paper.filename,
                abstract=paper.metadata.abstract or "æœªæå–åˆ°æ‘˜è¦",
                keywords=keywords_text,
                sections=sections_text,
                content=content
            )
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.llm.invoke([HumanMessage(content=prompt)])
            )
            keypoints = self._parse_response(response.content)

        # ä¿å­˜è¦ç‚¹
        if save:
            self._save_keypoints(paper, keypoints, output_dir)

        return keypoints

    def _prepare_paper_content(self, paper: ParsedPaper, max_chars: int = 8000) -> str:
        """
        å‡†å¤‡è®ºæ–‡å†…å®¹

        Args:
            paper: è®ºæ–‡å¯¹è±¡
            max_chars: æœ€å¤§å­—ç¬¦æ•°

        Returns:
            str: å¤„ç†åçš„è®ºæ–‡å†…å®¹
        """
        content_parts = []

        # æ·»åŠ æ ‡é¢˜
        if paper.metadata.title:
            content_parts.append(f"æ ‡é¢˜: {paper.metadata.title}")

        # æ·»åŠ æ‘˜è¦
        if paper.metadata.abstract:
            content_parts.append(f"æ‘˜è¦: {paper.metadata.abstract}")

        # æ·»åŠ ä¸»è¦ç« èŠ‚
        if paper.metadata.sections:
            content_parts.append("\nä¸»è¦ç« èŠ‚:")
            for section_name, section_content in paper.metadata.sections.items():
                content_parts.append(f"\n{section_name}:\n{section_content[:1500]}")

        # ç»„åˆå†…å®¹
        combined_content = "\n\n".join(content_parts)

        if len(combined_content) < max_chars:
            remaining_chars = max_chars - len(combined_content)
            full_text_sample = paper.full_text[:remaining_chars]
            combined_content += f"\n\nè®ºæ–‡æ­£æ–‡:\n{full_text_sample}"

        return combined_content[:max_chars]

    def _parse_response(self, response: str) -> Dict[str, List[str]]:
        """
        è§£æLLMå“åº”ï¼Œæå–JSONæ•°æ®ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰

        Args:
            response: LLMå“åº”æ–‡æœ¬

        Returns:
            Dict[str, List[str]]: è§£æåçš„è¦ç‚¹å­—å…¸
        """
        # å°è¯•æå–JSONéƒ¨åˆ†
        try:
            # æŸ¥æ‰¾JSONä»£ç å—
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            elif "```" in response:
                json_start = response.find("```") + 3
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()

            # è§£æJSON
            keypoints = json.loads(json_str)

            # éªŒè¯å¹¶ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            required_fields = ["innovations", "methods", "experiments", "conclusions", "contributions", "limitations"]
            for field in required_fields:
                if field not in keypoints:
                    keypoints[field] = []
                elif not isinstance(keypoints[field], list):
                    keypoints[field] = [str(keypoints[field])]

            return keypoints

        except Exception as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response}")
            return self._get_empty_keypoints()

    def _get_empty_keypoints(self) -> Dict[str, List[str]]:
        """è¿”å›ç©ºçš„è¦ç‚¹ç»“æ„"""
        return get_empty_keypoints() if LANGCHAIN_V2_AVAILABLE else {
            "innovations": [],
            "methods": [],
            "experiments": [],
            "conclusions": [],
            "contributions": [],
            "limitations": []
        }

    def _save_keypoints(
        self,
        paper: ParsedPaper,
        keypoints: Dict[str, List[str]],
        output_dir: Optional[Path] = None
    ):
        """
        ä¿å­˜è¦ç‚¹åˆ°æ–‡ä»¶

        Args:
            paper: è®ºæ–‡å¯¹è±¡
            keypoints: æå–çš„è¦ç‚¹
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir = output_dir or settings.keypoints_output_dir
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆå»æ‰.pdfï¼‰ä½œä¸ºè¾“å‡ºæ–‡ä»¶åï¼Œä¿å­˜ä¸ºmarkdownæ ¼å¼
        output_filename = Path(paper.filename).stem + "_keypoints.md"
        output_path = output_dir / output_filename

        # æ„å»ºmarkdownæ ¼å¼å†…å®¹
        from datetime import datetime
        md_content = f"""# è®ºæ–‡æ ¸å¿ƒè¦ç‚¹æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|------|------|
| è®ºæ–‡æ ‡é¢˜ | {paper.metadata.title or paper.filename} |
| æ–‡ä»¶å | `{paper.filename}` |
| é¡µæ•° | {paper.page_count} |
| ç”Ÿæˆæ—¶é—´ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |

---

"""

        # å†™å…¥å„ä¸ªç±»åˆ«
        field_names = {
            "innovations": "ğŸ”¥ æ ¸å¿ƒåˆ›æ–°ç‚¹",
            "methods": "ğŸ”§ ä¸»è¦æ–¹æ³•ä¸æŠ€æœ¯",
            "experiments": "ğŸ§ª å®éªŒè®¾è®¡ä¸è¯„ä¼°",
            "conclusions": "ğŸ’¡ ä¸»è¦ç»“è®º",
            "contributions": "ğŸ¯ å­¦æœ¯è´¡çŒ®",
            "limitations": "âš ï¸ å±€é™æ€§"
        }

        for field, display_name in field_names.items():
            md_content += f"## {display_name}\n\n"
            items = keypoints.get(field, [])
            if items:
                for i, item in enumerate(items, 1):
                    md_content += f"{i}. {item}\n"
            else:
                md_content += "_æœªæå–åˆ°ç›¸å…³å†…å®¹_\n"
            md_content += "\n"

        md_content += """---

*æ­¤æŠ¥å‘Šç”±é™¢å£«çº§ç§‘ç ”æ™ºèƒ½åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆ*
"""

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md_content)

    def batch_extract_keypoints(
        self,
        papers: List[ParsedPaper],
        output_dir: Optional[Path] = None
    ) -> List[Dict[str, List[str]]]:
        """
        æ‰¹é‡æå–è¦ç‚¹

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            List[Dict[str, List[str]]]: è¦ç‚¹åˆ—è¡¨
        """
        all_keypoints = []

        for i, paper in enumerate(papers, 1):
            print(f"æ­£åœ¨æå–ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡çš„è¦ç‚¹...")
            try:
                keypoints = self.extract_keypoints(paper, save=True, output_dir=output_dir)
                all_keypoints.append(keypoints)
                print(f"âœ“ å®Œæˆ: {paper.filename}")
            except Exception as e:
                print(f"âœ— å¤±è´¥: {paper.filename} - {e}")
                all_keypoints.append(self._get_empty_keypoints())

        return all_keypoints

    def generate_summary_report(
        self,
        keypoints: Dict[str, List[str]],
        paper_title: str
    ) -> str:
        """
        ç”Ÿæˆè¦ç‚¹æ‘˜è¦æŠ¥å‘Š

        Args:
            keypoints: æå–çš„è¦ç‚¹
            paper_title: è®ºæ–‡æ ‡é¢˜

        Returns:
            str: æ‘˜è¦æŠ¥å‘Š
        """
        report_lines = [
            f"# {paper_title}",
            "",
            "## æ ¸å¿ƒåˆ›æ–°ç‚¹"
        ]

        for item in keypoints.get("innovations", []):
            report_lines.append(f"- {item}")

        report_lines.extend([
            "",
            "## ä¸»è¦æ–¹æ³•"
        ])

        for item in keypoints.get("methods", []):
            report_lines.append(f"- {item}")

        report_lines.extend([
            "",
            "## ä¸»è¦ç»“è®º"
        ])

        for item in keypoints.get("conclusions", []):
            report_lines.append(f"- {item}")

        return "\n".join(report_lines)


def extract_keypoints_from_pdf(pdf_path: str) -> Dict[str, List[str]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»PDFæ–‡ä»¶æå–è¦ç‚¹

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„

    Returns:
        Dict[str, List[str]]: æå–çš„è¦ç‚¹
    """
    from src.pdf_parser import PDFParser

    # è§£æPDF
    parser = PDFParser()
    paper = parser.parse_pdf(pdf_path)

    # æå–è¦ç‚¹
    extractor = KeypointExtractor()
    keypoints = extractor.extract_keypoints(paper)

    return keypoints
