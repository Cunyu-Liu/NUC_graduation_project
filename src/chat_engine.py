"""AI èŠå¤©å¼•æ“ - v4.2é™¢å£«ç‰ˆ
æ”¯æŒæµå¼è¾“å‡ºã€ä¸Šä¸‹æ–‡ç®¡ç†ã€å·¥å…·è°ƒç”¨ã€å¤šæ¨¡æ€äº¤äº’
"""
import os
import json
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

# LangChain å¯¼å…¥
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    from langchain_core.tools import tool
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    HumanMessage = None
    AIMessage = None
    SystemMessage = None

# å‘é‡å­˜å‚¨å¯¼å…¥
try:
    from src.vector_store import get_vector_store_manager
    VECTOR_STORE_AVAILABLE = True
except ImportError:
    VECTOR_STORE_AVAILABLE = False

# è”ç½‘æœç´¢å¯¼å…¥
try:
    from src.web_search import get_search_engine, WebSearchEngine
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    WEB_SEARCH_AVAILABLE = False
    WebSearchEngine = None


class MessageType(Enum):
    """æ¶ˆæ¯ç±»å‹"""
    TEXT = "text"
    CODE = "code"
    IMAGE = "image"
    FILE = "file"
    TOOL_CALL = "tool_call"
    TOOL_RESULT = "tool_result"


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str  # "user", "assistant", "system"
    content: str
    message_type: MessageType = MessageType.TEXT
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    references: List[Dict[str, Any]] = field(default_factory=list)
    tool_calls: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class ChatContext:
    """èŠå¤©ä¸Šä¸‹æ–‡"""
    chat_id: str
    messages: List[ChatMessage] = field(default_factory=list)
    model: str = "glm-4-plus"
    temperature: float = 0.7
    max_tokens: int = 4000
    system_prompt: Optional[str] = None
    connected_papers: List[int] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    
    def add_message(self, message: ChatMessage):
        """æ·»åŠ æ¶ˆæ¯"""
        self.messages.append(message)
        self.updated_at = datetime.now()
        
        # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆä¿ç•™æœ€è¿‘20è½®ï¼‰
        if len(self.messages) > 40:
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘æ¶ˆæ¯
            system_msgs = [m for m in self.messages if m.role == "system"]
            recent_msgs = self.messages[-38:]
            self.messages = system_msgs + recent_msgs
    
    def to_langchain_messages(self) -> List:
        """è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼"""
        messages = []
        
        # ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            messages.append(SystemMessage(content=self.system_prompt))
        
        # å†å²æ¶ˆæ¯
        for msg in self.messages[-20:]:  # åªä½¿ç”¨æœ€è¿‘20æ¡
            if msg.role == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                messages.append(AIMessage(content=msg.content))
        
        return messages
    
    def clear(self):
        """æ¸…ç©ºä¸Šä¸‹æ–‡"""
        self.messages = []
        self.updated_at = datetime.now()


class ChatEngine:
    """AI èŠå¤©å¼•æ“"""
    
    # ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
    DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”åŠ©æ‰‹ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜è¿›è¡Œæ–‡çŒ®åˆ†æã€ç ”ç©¶è®¾è®¡å’Œå­¦æœ¯å†™ä½œã€‚

ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š
1. æ·±åº¦æ–‡çŒ®è§£è¯»å’Œåˆ†æ
2. ç ”ç©¶ç©ºç™½è¯†åˆ«å’Œå»ºè®®
3. å®éªŒè®¾è®¡å’Œæ–¹æ³•è®ºæŒ‡å¯¼
4. ä»£ç å®ç°å’Œç®—æ³•è§£é‡Š
5. å­¦æœ¯å†™ä½œå’Œæ¶¦è‰²

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
- å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ·±åº¦
- å¼•ç”¨ç›¸å…³è®ºæ–‡æ—¶è¦å‡†ç¡®
- å¯¹äºä¸ç¡®å®šçš„å†…å®¹è¦è¯šå®è¯´æ˜
- é¼“åŠ±æ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ›æ–°

å½“å‰æ—¶é—´ï¼š{current_time}"""

    def __init__(self, llm_config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–èŠå¤©å¼•æ“
        
        Args:
            llm_config: LLMé…ç½®
        """
        self.llm_config = llm_config or {}
        self.contexts: Dict[str, ChatContext] = {}
        self.db_manager = None
        
        # åˆå§‹åŒ– LLM
        if LANGCHAIN_AVAILABLE:
            self.llm = self._create_llm()
        else:
            self.llm = None
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        if VECTOR_STORE_AVAILABLE:
            self.vector_store = get_vector_store_manager()
        else:
            self.vector_store = None
        
        # åˆå§‹åŒ–æœç´¢å¼•æ“
        if WEB_SEARCH_AVAILABLE:
            self.search_engine = get_search_engine()
        else:
            self.search_engine = None
    
    def _create_llm(self, model: Optional[str] = None) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain æœªå®‰è£…")
        
        return ChatOpenAI(
            model=model or self.llm_config.get('model', 'glm-4-plus'),
            api_key=self.llm_config.get('api_key') or os.getenv('GLM_API_KEY'),
            base_url=self.llm_config.get('base_url') or os.getenv('GLM_BASE_URL'),
            temperature=self.llm_config.get('temperature', 0.7),
            max_tokens=self.llm_config.get('max_tokens', 4000),
            streaming=True,
            request_timeout=120
        )
    
    def create_context(self, 
                      chat_id: Optional[str] = None,
                      model: str = "glm-4-plus",
                      temperature: float = 0.7,
                      system_prompt: Optional[str] = None,
                      connected_papers: Optional[List[int]] = None) -> ChatContext:
        """
        åˆ›å»ºæ–°çš„èŠå¤©ä¸Šä¸‹æ–‡
        
        Args:
            chat_id: èŠå¤©IDï¼ˆå¯é€‰ï¼‰
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            connected_papers: å…³è”çš„è®ºæ–‡IDåˆ—è¡¨
            
        Returns:
            ChatContext å®ä¾‹
        """
        if chat_id is None:
            chat_id = f"chat_{datetime.now().strftime('%Y%m%d%H%M%S%f')}"
        
        # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        if system_prompt is None:
            system_prompt = self.DEFAULT_SYSTEM_PROMPT.format(
                current_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            )
        
        context = ChatContext(
            chat_id=chat_id,
            model=model,
            temperature=temperature,
            system_prompt=system_prompt,
            connected_papers=connected_papers or []
        )
        
        self.contexts[chat_id] = context
        return context
    
    def get_context(self, chat_id: str) -> Optional[ChatContext]:
        """è·å–èŠå¤©ä¸Šä¸‹æ–‡"""
        return self.contexts.get(chat_id)
    
    def delete_context(self, chat_id: str) -> bool:
        """åˆ é™¤èŠå¤©ä¸Šä¸‹æ–‡"""
        if chat_id in self.contexts:
            del self.contexts[chat_id]
            return True
        return False
    
    def clear_context(self, chat_id: str) -> bool:
        """æ¸…ç©ºèŠå¤©ä¸Šä¸‹æ–‡ä¸­çš„æ¶ˆæ¯"""
        context = self.contexts.get(chat_id)
        if context:
            context.clear()
            return True
        return False
    
    async def chat_stream(self, 
                         chat_id: str,
                         message: str,
                         use_rag: bool = True,
                         use_web_search: bool = False,
                         search_papers: bool = True,
                         files: Optional[List[Dict]] = None) -> AsyncGenerator[str, None]:
        """
        æµå¼èŠå¤©
        
        Args:
            chat_id: èŠå¤©ID
            message: ç”¨æˆ·æ¶ˆæ¯
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG
            use_web_search: æ˜¯å¦ä½¿ç”¨è”ç½‘æœç´¢
            search_papers: æ˜¯å¦æœç´¢ç›¸å…³è®ºæ–‡
            files: ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹åˆ—è¡¨
            
        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        if not LANGCHAIN_AVAILABLE or not self.llm:
            yield "æŠ±æ­‰ï¼ŒèŠå¤©åŠŸèƒ½å½“å‰ä¸å¯ç”¨ã€‚"
            return
        
        # è·å–æˆ–åˆ›å»ºä¸Šä¸‹æ–‡
        context = self.get_context(chat_id)
        if not context:
            context = self.create_context(chat_id)
        
        # æ„å»ºå¢å¼ºæç¤ºè¯
        enhanced_message = message
        references = []
        context_parts = []
        
        # 1. è”ç½‘æœç´¢
        web_search_results = []
        if use_web_search and self.search_engine:
            try:
                web_search_results = self.search_engine.search(message, max_results=3)
                if web_search_results:
                    web_context = self.search_engine.format_results_for_llm(web_search_results)
                    context_parts.append(web_context)
                    print(f"ğŸŒ è”ç½‘æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(web_search_results)} æ¡ç»“æœ")
            except Exception as e:
                print(f"âš ï¸ è”ç½‘æœç´¢å¤±è´¥: {e}")
        
        # 2. RAGï¼šæ£€ç´¢ç›¸å…³è®ºæ–‡
        if use_rag and self.vector_store and self.vector_store.is_available():
            try:
                search_results = []
                connected_papers = context.connected_papers if context else []
                
                # å¦‚æœæœ‰ç”¨æˆ·æŒ‡å®šçš„å…³è”è®ºæ–‡ï¼Œä¼˜å…ˆåœ¨è¿™äº›è®ºæ–‡ä¸­æœç´¢
                if connected_papers:
                    try:
                        search_results = self.vector_store.search(
                            message, 
                            top_k=5,
                            paper_ids=connected_papers
                        )
                        print(f"ğŸ” åœ¨ {len(connected_papers)} ç¯‡å…³è”è®ºæ–‡ä¸­æœç´¢ï¼Œæ‰¾åˆ° {len(search_results)} ç¯‡ç›¸å…³è®ºæ–‡")
                    except Exception as e:
                        print(f"âš ï¸ å…³è”è®ºæ–‡æœç´¢å¤±è´¥: {e}")
                
                # å¦‚æœå…³è”è®ºæ–‡ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œæˆ–è€…æ²¡æœ‰æŒ‡å®šå…³è”è®ºæ–‡ï¼Œåˆ™åœ¨å…¨éƒ¨è®ºæ–‡ä¸­æœç´¢
                if not search_results:
                    search_results = self.vector_store.search(message, top_k=3)
                    print(f"ğŸ” åœ¨å…¨éƒ¨è®ºæ–‡ä¸­æœç´¢ï¼Œæ‰¾åˆ° {len(search_results)} ç¯‡ç›¸å…³è®ºæ–‡")
                
                if search_results:
                    # æ„å»ºä¸Šä¸‹æ–‡æç¤ºè¯
                    if connected_papers:
                        paper_header = f"ã€æ‚¨çš„è®ºæ–‡åº“ã€‘ï¼ˆä¼˜å…ˆä»æ‚¨å…³è”çš„ {len(connected_papers)} ç¯‡è®ºæ–‡ä¸­æ£€ç´¢ï¼‰\n\n"
                    else:
                        paper_header = "ã€æ‚¨çš„è®ºæ–‡åº“ã€‘\n\n"
                    
                    paper_context = paper_header + "\n\n".join([
                        f"è®ºæ–‡ {i+1}: {r.title}\n{r.abstract[:500]}..."
                        for i, r in enumerate(search_results)
                    ])
                    context_parts.append(paper_context)
                    
                    references = [
                        {"paper_id": r.paper_id, "title": r.title, "distance": r.distance}
                        for r in search_results
                    ]
            except Exception as e:
                print(f"RAG æœç´¢å¤±è´¥: {e}")
        
        # 3. å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        if files and len(files) > 0:
            file_contexts = []
            for i, file_info in enumerate(files, 1):
                filename = file_info.get('filename', f'file_{i}')
                content = file_info.get('content', '')
                file_type = file_info.get('content_type', 'unknown')
                
                if content:
                    file_contexts.append(
                        f"ã€æ–‡ä»¶ {i}: {filename}ã€‘\n"
                        f"ç±»å‹: {file_type}\n"
                        f"å†…å®¹:\n{content[:5000]}"  # é™åˆ¶æ¯ä¸ªæ–‡ä»¶é•¿åº¦
                    )
            
            if file_contexts:
                files_context = "ã€ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ã€‘\n\n" + "\n\n---\n\n".join(file_contexts)
                context_parts.append(files_context)
                print(f"ğŸ“ å·²å¤„ç† {len(files)} ä¸ªä¸Šä¼ æ–‡ä»¶")
        
        # ç»„åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
        if context_parts:
            enhanced_message = "\n\n".join(context_parts) + f"\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{message}"
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_msg = ChatMessage(
            role="user",
            content=message,
            references=references
        )
        context.add_message(user_msg)
        
        # å‡†å¤‡æ¶ˆæ¯
        messages = context.to_langchain_messages()
        messages.append(HumanMessage(content=enhanced_message))
        
        # æµå¼ç”Ÿæˆ
        full_response = ""
        try:
            async for chunk in self.llm.astream(messages):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += content
                yield content
        except Exception as e:
            yield f"\n\n[é”™è¯¯: {str(e)}]"
            return
        
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
        assistant_msg = ChatMessage(
            role="assistant",
            content=full_response,
            references=references
        )
        context.add_message(assistant_msg)
    
    async def chat(self, 
                  chat_id: str,
                  message: str,
                  use_rag: bool = True) -> Dict[str, Any]:
        """
        éæµå¼èŠå¤©
        
        Args:
            chat_id: èŠå¤©ID
            message: ç”¨æˆ·æ¶ˆæ¯
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG
            
        Returns:
            å®Œæ•´å“åº”
        """
        full_response = ""
        async for chunk in self.chat_stream(chat_id, message, use_rag):
            full_response += chunk
        
        return {
            "content": full_response,
            "chat_id": chat_id,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_chat_history(self, chat_id: str) -> List[Dict[str, Any]]:
        """
        è·å–èŠå¤©å†å²
        
        Args:
            chat_id: èŠå¤©ID
            
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        context = self.get_context(chat_id)
        if not context:
            return []
        
        return [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat(),
                "references": msg.references
            }
            for msg in context.messages
        ]
    
    def list_chats(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰èŠå¤©ä¼šè¯
        
        Returns:
            èŠå¤©ä¼šè¯åˆ—è¡¨
        """
        return [
            {
                "chat_id": chat_id,
                "message_count": len(context.messages),
                "model": context.model,
                "created_at": context.created_at.isoformat(),
                "updated_at": context.updated_at.isoformat(),
                "preview": context.messages[-1].content[:50] + "..." if context.messages else ""
            }
            for chat_id, context in self.contexts.items()
        ]
    
    def generate_chat_title(self, first_message: str) -> str:
        """
        æ ¹æ®ç¬¬ä¸€æ¡æ¶ˆæ¯ç”ŸæˆèŠå¤©æ ‡é¢˜
        
        Args:
            first_message: ç¬¬ä¸€æ¡æ¶ˆæ¯
            
        Returns:
            ç”Ÿæˆçš„æ ‡é¢˜
        """
        # ç®€å•è§„åˆ™ç”Ÿæˆæ ‡é¢˜
        title = first_message[:30] if len(first_message) <= 30 else first_message[:27] + "..."
        return title
    
    async def analyze_papers(self, 
                            chat_id: str,
                            paper_ids: List[int],
                            analysis_type: str = "summary") -> AsyncGenerator[str, None]:
        """
        åˆ†ææŒ‡å®šè®ºæ–‡
        
        Args:
            chat_id: èŠå¤©ID
            paper_ids: è®ºæ–‡IDåˆ—è¡¨
            analysis_type: åˆ†æç±»å‹
            
        Yields:
            æµå¼å“åº”
        """
        if not self.db_manager:
            yield "é”™è¯¯ï¼šæ•°æ®åº“ç®¡ç†å™¨æœªé…ç½®"
            return
        
        # è·å–è®ºæ–‡æ•°æ®
        papers = []
        for pid in paper_ids:
            paper = self.db_manager.get_paper(pid)
            if paper:
                papers.append(paper)
        
        if not papers:
            yield "æœªæ‰¾åˆ°æŒ‡å®šçš„è®ºæ–‡"
            return
        
        # æ„å»ºåˆ†ææç¤ºè¯
        if analysis_type == "summary":
            prompt = f"è¯·å¯¹ä»¥ä¸‹ {len(papers)} ç¯‡è®ºæ–‡è¿›è¡Œç»¼åˆåˆ†æå’Œæ€»ç»“ï¼š\n\n"
        elif analysis_type == "compare":
            prompt = f"è¯·å¯¹æ¯”åˆ†æä»¥ä¸‹ {len(papers)} ç¯‡è®ºæ–‡çš„æ–¹æ³•å’Œç»“æœï¼š\n\n"
        elif analysis_type == "gaps":
            prompt = f"è¯·åŸºäºä»¥ä¸‹ {len(papers)} ç¯‡è®ºæ–‡è¯†åˆ«ç ”ç©¶ç©ºç™½ï¼š\n\n"
        else:
            prompt = f"è¯·åˆ†æä»¥ä¸‹ {len(papers)} ç¯‡è®ºæ–‡ï¼š\n\n"
        
        for i, paper in enumerate(papers, 1):
            prompt += f"è®ºæ–‡ {i}:\n"
            prompt += f"æ ‡é¢˜: {paper.get('title', 'æœªçŸ¥')}\n"
            prompt += f"æ‘˜è¦: {paper.get('abstract', 'æ— æ‘˜è¦')[:500]}...\n\n"
        
        async for chunk in self.chat_stream(chat_id, prompt, use_rag=False):
            yield chunk
    
    async def generate_literature_review(self, 
                                        chat_id: str,
                                        topic: str,
                                        paper_ids: Optional[List[int]] = None) -> AsyncGenerator[str, None]:
        """
        ç”Ÿæˆæ–‡çŒ®ç»¼è¿°
        
        Args:
            chat_id: èŠå¤©ID
            topic: ç ”ç©¶ä¸»é¢˜
            paper_ids: å¯é€‰çš„è®ºæ–‡IDåˆ—è¡¨
            
        Yields:
            æµå¼å“åº”
        """
        prompt = f"è¯·é’ˆå¯¹ä¸»é¢˜ã€Œ{topic}ã€ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ–‡çŒ®ç»¼è¿°ã€‚"
        
        if paper_ids and self.db_manager:
            prompt += "\n\nåŸºäºä»¥ä¸‹è®ºæ–‡ï¼š\n"
            for pid in paper_ids:
                paper = self.db_manager.get_paper(pid)
                if paper:
                    prompt += f"- {paper.get('title', 'æœªçŸ¥')}\n"
        
        prompt += """

æ–‡çŒ®ç»¼è¿°ç»“æ„ï¼š
1. ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰
2. ç›¸å…³å·¥ä½œçš„ç³»ç»Ÿå›é¡¾
3. æ–¹æ³•è®ºçš„æ¼”è¿›
4. ç°æœ‰ç ”ç©¶çš„ä¸è¶³
5. æœªæ¥ç ”ç©¶æ–¹å‘

è¯·ç”Ÿæˆç»¼è¿°ï¼š"""
        
        async for chunk in self.chat_stream(chat_id, prompt, use_rag=True):
            yield chunk


# å…¨å±€å¼•æ“å®ä¾‹
_chat_engine = None

def get_chat_engine(llm_config: Optional[Dict[str, Any]] = None, 
                   db_manager=None) -> ChatEngine:
    """è·å–èŠå¤©å¼•æ“å®ä¾‹"""
    global _chat_engine
    if _chat_engine is None:
        _chat_engine = ChatEngine(llm_config=llm_config)
        _chat_engine.db_manager = db_manager
    return _chat_engine
